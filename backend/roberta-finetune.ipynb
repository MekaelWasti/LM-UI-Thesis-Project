{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModelWithHeads\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = RobertaModelWithHeads.from_pretrained(\"roberta-base\")\n",
    "model.load_adapter(\"AdapterHub/roberta-base-pf-drop\", source=\"hf\")\n",
    "model.train_adapter(\"drop\", True)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set Adapter and NER head as trainable\n",
    "model.add_classification_head('ner_head', num_labels=13)\n",
    "for param in model.heads['ner_head'].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2721, -0.3810,  0.0575,  0.0949, -0.0121, -0.2263,  0.2611,  0.0819,\n",
      "          0.0600, -0.1109,  0.3534,  0.0623, -0.0934]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"What we feeling outside\"]\n",
    "# Tokenize input\n",
    "# encoding = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "encoding = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=97)\n",
    "inputs = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_ids=inputs, attention_mask=attention_mask, output_attentions=False)\n",
    "logits = outputs.logits\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ID = {\n",
    "    \"O\": 0,\n",
    "    \"B-TITLE\": 1,\n",
    "    \"I-TITLE\": 2,\n",
    "    \"B-NAME\": 3,\n",
    "    \"I-NAME\": 4,\n",
    "    \"B-ADDRESS\": 5,\n",
    "    \"I-ADDRESS\": 6,\n",
    "    \"B-CITY\": 7,\n",
    "    \"I-CITY\": 8,\n",
    "    \"B-COUNTRY\": 9,\n",
    "    \"I-COUNTRY\": 10,\n",
    "    \"B-ARITHMETIC\": 11,\n",
    "    \"I-ARITHMETIC\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'My', 'Ġhappily', 'Ġname', 'Ġis', 'ĠMrs', '.', 'ĠEmily', 'ĠWatson', ',', 'ĠI', 'Ġreside', 'Ġat', 'Ġ22', 'ĠBaker', 'ĠStreet', ',', 'ĠLondon', '.\"', 'Ġ', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[    0,  2387, 16534,   766,    16,  3801,     4,  7770,  5399,     6,\n",
       "             38, 23773,    23,   820,  5643,   852,     6,   928,    72,  1437,\n",
       "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
       "          [ 0,  2],\n",
       "          [ 3, 10],\n",
       "          [11, 15],\n",
       "          [16, 18],\n",
       "          [19, 22],\n",
       "          [22, 23],\n",
       "          [24, 29],\n",
       "          [30, 36],\n",
       "          [36, 37],\n",
       "          [38, 39],\n",
       "          [40, 46],\n",
       "          [47, 49],\n",
       "          [50, 52],\n",
       "          [53, 58],\n",
       "          [59, 65],\n",
       "          [65, 66],\n",
       "          [67, 73],\n",
       "          [73, 75],\n",
       "          [76, 76],\n",
       "          [ 0,  0]]])},\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizerF = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "def labelizeData(dataLine):\n",
    "    prompt,BOI = dataLine.split('||')\n",
    "\n",
    "    # tokenized = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256, is_split_into_words=True)\n",
    "    # tokenized = tokenizerF(prompt, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=97)\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    tokenized_words = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "    # tokenized_words = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0], skip_special_tokens=True)\n",
    "    # tokenized_words = [token[1:] if token.startswith('Ġ') else token for token in tokenized_words]\n",
    "    print(tokenized_words)\n",
    "    BOI = BOI.replace('\\n',\"\")\n",
    "    BOI = BOI.replace('\"',\"\")\n",
    "    BOI = BOI.replace('.',\"\")\n",
    "    BOI = BOI.strip()\n",
    "    BOI = BOI.split(',')\n",
    "    BOI = dict([x.split(':') for x in BOI])\n",
    "    # labels = [BOI[x] if x in BOI else \"O\" for ind,x in enumerate(tokenized_words)]\n",
    "    labels = [label_to_ID[BOI[x]] if x in BOI else 0 for x in tokenized_words]\n",
    "    labels += [0] * (97 - len(labels))\n",
    "    return tokenized, labels\n",
    "\n",
    "res = labelizeData('''My happily name is Mrs. Emily Watson, I reside at 22 Baker Street, London.\" || \"Mrs.:B-TITLE,Emily:B-NAME,Watson:I-NAME,22:B-ADDRESS,Baker:I-ADDRESS,Street:I-ADDRESS,London:I-ADDRESS''')\n",
    "res[0],res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'My',\n",
       " 'ĠMek',\n",
       " 'ael',\n",
       " 'Ġhappily',\n",
       " 'Ġname',\n",
       " 'Ġis',\n",
       " 'ĠMrs',\n",
       " '.',\n",
       " 'ĠEmily',\n",
       " 'ĠWatson',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġreside',\n",
       " 'Ġat',\n",
       " 'Ġ22',\n",
       " 'ĠBaker',\n",
       " 'ĠStreet',\n",
       " ',',\n",
       " 'ĠLondon',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Initialize the fast tokenizer\n",
    "tokenizerF = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "# Your sentence\n",
    "sentence = \"My Mekael happily name is Mrs. Emily Watson, I reside at 22 Baker Street, London.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_input = tokenizerF(sentence)  # Do not use is_split_into_words if `sentence` is a single string\n",
    "tokens = tokenizerF.convert_ids_to_tokens(tokenized_input[\"input_ids\"], skip_special_tokens=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: None\n",
      "Mad: 0\n",
      "ame: 0\n",
      "ĠCharlotte: 1\n",
      "ĠWilson: 2\n",
      "Ġhere: 3\n",
      ",: 4\n",
      "Ġfrom: 5\n",
      "Ġ33: 6\n",
      "ĠMaple: 7\n",
      "ĠDrive: 8\n",
      ",: 9\n",
      "ĠVienna: 10\n",
      ".: 11\n",
      "</s>: None\n",
      "<s>: -100\n",
      "Mad: 1\n",
      "ame: 1\n",
      "ĠCharlotte: 1\n",
      "ĠWilson: 2\n",
      "Ġhere: 2\n",
      ",: 0\n",
      "Ġfrom: 0\n",
      "Ġ33: 0\n",
      "ĠMaple: 0\n",
      "ĠDrive: 3\n",
      ",: 4\n",
      "ĠVienna: 4\n",
      ".: 4\n",
      "</s>: -100\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "sentence = \"Madame Charlotte Wilson here, from 33 Maple Drive, Vienna.\"\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Madame Charlotte Wilson here, from 33 Maple Drive, Vienna.\"\n",
    "\n",
    "# Tokenize the text and get word IDs (word_ids will indicate which word each token corresponds to)\n",
    "tokenized_input = tokenizer(sentence)\n",
    "\n",
    "# Print out the tokens and corresponding word IDs for examination\n",
    "for token, word_id in zip(tokenized_input.tokens(), tokenized_input.word_ids()):\n",
    "    print(f\"{token}: {word_id}\")\n",
    "\n",
    "# From the output, you'll be able to see which words are split into subtokens\n",
    "# and which punctuation marks are treated as separate tokens.\n",
    "\n",
    "# Let's assume that upon examination, you find that \"Madame\" is split and \",\" is treated as separate.\n",
    "# Adjust the word-level labels accordingly. If a word is split into subtokens, each subtoken should\n",
    "# have the same label as the original word. Punctuation typically receives an \"O\" label.\n",
    "\n",
    "# Revised word-level labels with additional \"O\" for the punctuation\n",
    "word_labels = [\"B-PER\", \"B-PER\", \"I-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ADD\", \"I-ADD\", \"I-ADD\", \"I-ADD\", \"O\"]\n",
    "\n",
    "\n",
    "# Check if the number of tokens that correspond to words matches the number of labels\n",
    "word_ids = tokenized_input.word_ids()\n",
    "actual_word_tokens = [word_id for word_id in word_ids if word_id is not None]\n",
    "\n",
    "if len(actual_word_tokens) != len(word_labels):\n",
    "    print(f\"Number of word tokens: {len(actual_word_tokens)}\")\n",
    "    print(f\"Number of labels: {len(word_labels)}\")\n",
    "    raise ValueError(\"The number of word-level labels does not match the number of word tokens.\")\n",
    "\n",
    "# Label map to convert labels to numerical IDs\n",
    "label_map = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ADD\": 3, \"I-ADD\": 4}\n",
    "\n",
    "# Convert word-level labels to numerical labels\n",
    "numerical_labels = [label_map[label] for label in word_labels]\n",
    "\n",
    "# Initialize a list to keep our aligned labels\n",
    "aligned_labels = []\n",
    "\n",
    "# Iterate over the tokens and their corresponding word IDs\n",
    "for word_id in word_ids:\n",
    "    if word_id is None:\n",
    "        aligned_labels.append(-100)\n",
    "    else:\n",
    "        aligned_labels.append(numerical_labels[word_id])\n",
    "\n",
    "# Combine the tokens and their labels\n",
    "token_label_pairs = [(token, label) for token, label in zip(tokens, aligned_labels)]\n",
    "\n",
    "# Print the tokens and their labels\n",
    "for token, label in token_label_pairs:\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', -100)\n",
      "('Mad', 1)\n",
      "('ame', 1)\n",
      "('ĠCharlotte', 1)\n",
      "('ĠWilson', 2)\n",
      "('Ġhere', 2)\n",
      "(',', 0)\n",
      "('Ġfrom', 0)\n",
      "('Ġ33', 0)\n",
      "('ĠMaple', 0)\n",
      "('ĠDrive', 3)\n",
      "(',', 4)\n",
      "('ĠVienna', 4)\n",
      "('.', 4)\n",
      "('</s>', -100)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and get the word IDs\n",
    "tokenized_input = tokenizer(sentence, is_split_into_words=False, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
    "\n",
    "# Combine the tokens and their labels\n",
    "token_label_pairs = [(token, label) for token, label in zip(tokens, aligned_labels)]\n",
    "\n",
    "# Print them out\n",
    "for pair in token_label_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ner_dataset.txt', 'r') as data:\n",
    "    dataset = data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'My', 'Ġname', 'Ġis', 'ĠMrs', '.', 'ĠEmily', 'ĠWatson', ',', 'ĠI', 'Ġreside', 'Ġat', 'Ġ22', 'ĠBaker', 'ĠStreet', ',', 'ĠLondon', '.\"', 'Ġ', '</s>']\n",
      "['<s>', 'My', 'Ġname', 'Ġis', 'ĠMrs', '.', 'ĠEmily', 'ĠWatson', ',', 'ĠI', 'Ġreside', 'Ġat', 'Ġ22', 'ĠBaker', 'ĠStreet', ',', 'ĠLondon', '.\"', 'Ġ', '</s>']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "align_labels_with_tokens() missing 1 required positional argument: 'label_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mekae\\Desktop\\CS\\ML-DL\\My Projects\\Thesis Project II\\backend\\roberta-finetune.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(x.word_ids())\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m word_labels \u001b[39m=\u001b[39m labelizeData(line)[\u001b[39m1\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m aligned_labels \u001b[39m=\u001b[39m align_labels_with_tokens(word_labels, x\u001b[39m.\u001b[39;49mword_ids())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(aligned_labels)\n",
      "\u001b[1;31mTypeError\u001b[0m: align_labels_with_tokens() missing 1 required positional argument: 'label_map'"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = []\n",
    "labels_ID = []\n",
    "for line in dataset:\n",
    "\n",
    "    x = labelizeData(line)[0]\n",
    "    dataset_tokenized.append(x)\n",
    "    # print(x.word_ids())\n",
    "    word_labels = labelizeData(line)[1]\n",
    "    aligned_labels = align_labels_with_tokens(word_labels, x.word_ids())\n",
    "    # print(x)\n",
    "    print(aligned_labels)\n",
    "    labels_ID.append(aligned_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xL = labels_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1579"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokenized)\n",
    "# len(labels_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mekae\\Desktop\\CS\\ML-DL\\My Projects\\Thesis Project II\\backend\\roberta-finetune.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind,x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset_tokenized):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     y \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(x[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(y, labels_ID[ind]):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mi[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mi[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mekae/Desktop/CS/ML-DL/My%20Projects/Thesis%20Project%20II/backend/roberta-finetune.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# dataset_tokenized\n",
    "# xL = [x + [0] * (97 - len(x)) for x in xL]\n",
    "# xL\n",
    "# labels_ID = xL\n",
    "\n",
    "for ind,x in enumerate(dataset_tokenized):\n",
    "    y = tokenizer.convert_ids_to_tokens(x[\"input_ids\"][0])\n",
    "    for i in zip(y, labels_ID[ind]):\n",
    "        print(f'({i[0]}:{i[1]})', end=\",\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized_1 = dataset_tokenized[980:981]\n",
    "\n",
    "for ind,x in enumerate(dataset_tokenized_1):\n",
    "    toke = tokenizer.convert_ids_to_tokens(x['input_ids'][0], skip_special_tokens=True)\n",
    "    print(toke)\n",
    "    print(labels_ID[ind])\n",
    "    for t in range(len(toke)):\n",
    "        print(f'{labels_ID[ind][t]}:,{toke[t]}', end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset_tokenized, labels_ID, max_length=512):\n",
    "        self.tokenized_sentences = dataset_tokenized\n",
    "        self.labels = labels_ID\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Padding the sequences and labels manually\n",
    "        input_ids = self.tokenized_sentences[idx][\"input_ids\"]\n",
    "        attention_mask = self.tokenized_sentences[idx][\"attention_mask\"]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        # Padding manually to max_length\n",
    "        # input_ids = input_ids + [0] * (self.max_length - len(input_ids))\n",
    "        # attention_mask = attention_mask + [0] * (self.max_length - len(attention_mask))\n",
    "        labels = labels + [0] * (self.max_length - len(labels))\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0] * 512)\n",
    "x = x[None,:]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = NERDataset(dataset_tokenized, labels_ID)\n",
    "# dataloader = DataLoader(ner_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(ner_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "for batch in dataloader:\n",
    "        if steps > 1:\n",
    "                break\n",
    "        inputs = batch[\"input_ids\"].squeeze(1)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        batch_size, _ = labels.shape\n",
    "        labels = labels.view(batch_size, -1)[:, 0]\n",
    "\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask, task_name='ner_head', device=device)\n",
    "        \n",
    "        # print(tokenizer.convert_ids_to_tokens(outputs[0][0]))\n",
    "        logits = outputs.logits\n",
    "        print(logits.shape)\n",
    "        # predicted_label_ids = torch.argmax(logits, dim=1)\n",
    "        # print(predicted_label_ids)\n",
    "\n",
    "\n",
    "\n",
    "        # probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # Print probabilities of the first 5 tokens for inspection\n",
    "        # print(probabilities[:5])\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        # print(logits.shape)\n",
    "        # print(labels.shape)\n",
    "        # loss = loss_fn(logits, labels)\n",
    "\n",
    "        # loss.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[\"input_ids\"].squeeze(1)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
    "        labels = batch[\"labels\"]\n",
    "        labels=labels.view(32, 512)[:,0]\n",
    "\n",
    "        inputs.to(device)\n",
    "        attention_mask.to(device)\n",
    "        labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask, task_name='ner_head')\n",
    "        print(outputs[0].shape)\n",
    "        logits = outputs[0].view(-1, 13)\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        print(logits.shape)\n",
    "        print(labels.shape)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 300 == 0:\n",
    "            print(f'Step {steps}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for a progress bar\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = RobertaModelWithHeads.from_pretrained(\"roberta-base\")\n",
    "model.load_adapter(\"AdapterHub/roberta-base-pf-drop\", source=\"hf\")\n",
    "model.train_adapter(\"drop\", True)\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "ID_to_label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-TITLE\",\n",
    "    2: \"I-TITLE\",\n",
    "    3: \"B-NAME\",\n",
    "    4: \"I-NAME\",\n",
    "    5: \"B-ADDRESS\",\n",
    "    6: \"I-ADDRESS\",\n",
    "    7: \"B-CITY\",\n",
    "    8: \"I-CITY\",\n",
    "    9: \"B-COUNTRY\",\n",
    "    10: \"I-COUNTRY\",\n",
    "    11: \"B-ARITHMETIC\",\n",
    "    12: \"I-ARITHMETIC\"\n",
    "}\n",
    "\n",
    "label_to_ID = {\n",
    "    \"O\": 0,\n",
    "    \"B-TITLE\": 1,\n",
    "    \"I-TITLE\": 2,\n",
    "    \"B-NAME\": 3,\n",
    "    \"I-NAME\": 4,\n",
    "    \"B-ADDRESS\": 5,\n",
    "    \"I-ADDRESS\": 6,\n",
    "    \"B-CITY\": 7,\n",
    "    \"I-CITY\": 8,\n",
    "    \"B-COUNTRY\": 9,\n",
    "    \"I-COUNTRY\": 10,\n",
    "    \"B-ARITHMETIC\": 11,\n",
    "    \"I-ARITHMETIC\": 12,\n",
    "}\n",
    "\n",
    "ner_dataset = NERDataset(dataset_tokenized, labels_ID)\n",
    "# dataloader = DataLoader(ner_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(ner_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Set Adapter and NER head as trainable\n",
    "model.add_classification_head(\"ner_head\", num_labels=13)\n",
    "\n",
    "for param in model.heads['ner_head'].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_function = CrossEntropyLoss().to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for batch in tqdm(dataloader):  # assuming dataloader is your DataLoader\n",
    "        # Move batch tensors to the same device as the model\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        print(input_ids.shape)  # Expected: [32, 512]\n",
    "        print(attention_mask.shape)  # Expected: [32, 512]\n",
    "        print(labels.shape)  # Expected: [32, 512]\n",
    "        print(logits.shape)  # Expected: [32, 512]\n",
    "\n",
    "\n",
    "        # Reshape labels and logits for loss function\n",
    "        loss = loss_function(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        print(logits.shape)  # should print [32, 512, 13]\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForTokenClassification, RobertaTokenizer, AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdapterConfig\n",
    "from transformers import RobertaModelWithHeads\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=13).to(device)\n",
    "# model = RobertaModelWithHeads.from_pretrained(\"roberta-base\",num_labels=13).to(device)\n",
    "\n",
    "# model.load_adapter(\"AdapterHub/roberta-base-pf-drop\", source=\"hf\")\n",
    "# model.train_adapter(\"drop\")\n",
    "# model.add_classification_head(\"ner_head\", num_labels=13, adapter_name=\"drop\")\n",
    "\n",
    "\n",
    "# Define label mappings\n",
    "# ... (Your label_to_ID and ID_to_label mappings remain unchanged)\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "# ... (Your NERDataset and DataLoader code remains unchanged)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_function = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for batch in tqdm(dataloader):  # assuming dataloader is your DataLoader\n",
    "        # Move batch tensors to the same device as the model\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].squeeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "\n",
    "# Sample input text\n",
    "text = \"John Doe lives in New York.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "# Get the model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convert predicted token IDs to labels\n",
    "predicted_labels = [ID_to_label[id.item()] for id in predictions[0]]\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
